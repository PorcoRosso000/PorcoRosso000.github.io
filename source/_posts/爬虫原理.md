---
title: 爬虫原理
typora-root-url: 爬虫原理
abbrlink: b958062a
date: 2022-11-26 16:48:41
tags:
permalink:
---



## 爬虫原理

如果我们把互联网比作一张大的蜘蛛网,数据便是存放于蜘蛛网的各个节点,而爬虫就是一只小蜘蛛,沿着网络抓取自己的猎物(数据)爬虫指的是:向网络发起请求,获取资源后分析并提取有用数据的程序

从技术层面来说就是通过程序模拟浏览器请求站点的行为,吧站点返回HTML代码/JSON数据/二进制数据(图片,视频)爬到本地,进而提取自己需要的数据,存放起来使用

爬虫的基本流程:

模拟浏览器发送请求(获取网页代码)

提取有用的数据

存放于数据库或文件

反爬虫机制

检验数据头 User-Agent反爬虫机制

当我们使用浏览器访问网站的时候,浏览器会发送一小段信息给网站,我们称之为Request Headers, 在这个头部信息里面包含了本次访问的一些信息;其中最常被用到的一个信息叫做”User-Agent”.

网站可以通过User-Agent来判断用户是在使用什么浏览器访问,不同的浏览器的User-Agent是不一样的,但都有遵循一定的规则.

但是如果我们使用Python的Requests直接访问网站,处理网址不提供其他的信息,那么网站收到的User-Agent是空的.这个时候就知道我们不是使用浏览器访问的,于是它就可以拒绝我们的访问

  访问频率限制或检验

大多数情况下,我们遇到的是访问频率的限制.如果你访问太快了,网站就会认为你不是一个人.

这种情况下需要设定好的阈值,否则可能会误伤.

另外,还可以检索访问频率是否每次都相同,如果相同,那么久一定是爬虫了

蜜罐技术

蜜獾:来自于网络攻防战中,一方会故意设置一个或者几个服务器,故意留下漏洞,让另一方轻易的入侵进来.而这些故意设置的服务器,就是蜜罐,

里面可能安装了监控软件,用来监控入侵者,同时还能拖延	入侵者的时间.

在反爬虫机制中:网页上会故意留下一些人类看不到或者绝对不会点击的链接,

由于爬虫会从源代码中获取内容,所以爬虫可能会访问这样的链接,这个时候只要网站发现了IP访问这个链接,立刻永久封禁该IP + User-Agent + Mac地址等等,可以应用于访问者身份识别.这个时候,访问者即使更换IP,也没有办法访问这个网站了.给爬虫造成了非常大的访问障碍

应对方案:

设定Request Headers中的User-Agent

检查User-Agent是一种最简单的反爬虫机制,而通过设定Request Headers中User-Agent,可以突破这种机制.

设定随机访问频率

遇到访问频率限制可以采用设定随机访问频率的方式来突破

代理IP或者分布爬虫

另外,如果对页的爬虫的效率有要求,那就不能通过设定访问时间间隔来绕过频率检查了,可以采用代理IP或者分布式爬虫

1.如果通100个代理IP来访问100个页面,可以给网站造成一种有100个人,每人访问1页的错觉,自然就不会被限制访问了

2.分布式爬虫会部署到台服务器上,每个服务器上的爬虫统一从一个地方拿网址这样平均下来每个服务器访问网站的频率也就降低了.由于服务器是掌握在我们手上的,因此实现的爬虫会更加的稳定和高效

对于蜜罐技术可以采用定向爬虫

由于定向爬虫的爬虫轨迹是由我们来决定的,爬虫会访问哪些网址我们都是知道的.

因此即使网站中有蜜罐,定向爬虫也不一定会中招

通过cookie和验证码识别爬虫

场景:

1. 点击登录按钮之后发起post请求
2. Post请求中会携带登录之前录入的相关登录信息(用户名,密码)
3. 没有请求到对应的页码数据的原因:
   1. 发起第二次基于个人主页的页面请求的时候,服务器端并不知道本次请求是基于登录状态下的请求.
4. Cookie:用来让服务器daunt记录客户端的相关状态

解决方案:

手动处理

通过抓包工具来获取Cookie值,讲该值封装到Headers中

自动处理

Session会话对象

可以进行请求的发送

如果请求过程中产生了cookie,则cookie会被自动存储/携带在该Session对象中

操作步骤:

- 创建一个session对象:session=requests.Session()
- 是同session 	对象进行模拟登录post请求的发送(cookie就会被存储在session中)
- Session对象对于个人主页的get请求进行了发送(携带了cookie)