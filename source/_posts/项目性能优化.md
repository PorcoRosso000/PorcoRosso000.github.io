---
title: 项目性能优化
typora-root-url: 项目性能优化
tags: 性能优化
categories: 性能优化
abbrlink: e64cd6b8
date: 2023-12-27 18:46:38
permalink:
---



# 请求折叠工具类

## 背景

在大流量高并发场景下，我们项目通常会达到性能的瓶颈。这些瓶颈总结起来，大概有两种，**读瓶颈**，和**写瓶颈**。

这些瓶颈有个通用的解决方案，如果是不重要请求，可以无脑三板斧 **熔断**，**限流**，**降级**。接下来介绍重要的请求处理方式。

### 读瓶颈

读瓶颈比写瓶颈更容易出现，也更好解决。比如：

1. 首页的框架接口用户一进来就要访问
2. 商品详情页的接口
3. 商品列表的接口

针对这种读请求很多，导致达到了性能的瓶颈。需要去分析请求的链路，看看到底慢在了哪里。它的解决核心思想，就是让资源离用户更近。

这个近是广义的近，能让用户获取起来更快，链路更短，它就算近。常见的两种解决方案

#### 加缓存

加缓存可以说是常见的解决读瓶颈的方案了。因为**mysql**是查磁盘的慢一些，redis是查缓存的，更快一些，就可以相对的认为，资源**存redis**离用户更近。

![img](./9.jpg)

甚至redis经常会因为读请求太多，还会出现带宽被打满的情况。

这时候可以用到**本地JVM缓存**，这样资源离用户又更近了，请求打进Tomcat，直接就能在内存查，效率相当于一条cpu命令的效率，不需要网络开销了。

本地JVM存太多容易满，这时候最好是能识别出热点数据，做一个冷热分离。热数据存jvm，冷数据存redis或mysql。用jd开源的[hotkey](https://gitee.com/jd-platform-opensource/hotkey)热key探查，可以查出热点数据，推送到jvm进行缓存。

对于一些静态数据，可以做一个动静分离，甚至不用存在本地。比如商详的商品图，描述，标题。这些东西可以直接存到**nginx**去。通过openresty写lua脚本缓存相对静态的数据。

针对于商品图片的加载需要大量的带宽，我们服务器的机房会有比较大的压力，这些流量甚至不用打到本地机房，通过**cdn**的方式，直接分发到各个距离用户最近的cdn节点。

最最强大的杀招来了，资源离用户最近的地方，当然是**用户的客户端**。而且用户客户端是天然的分布式节点，每个用户都有一个自己的客户端。就比如微信消息，就存在用户的手机客户端。有一些消耗计算的工作内容，先让客户端做好了传给服务端。

#### 预处理

一个资源从写入到展示给前端，实际上是需要一些列的加工步骤的。这些加工是耗时的没法避免的。

要么后端做，要么前端来做。

![img](./image.png)

耗时不会消失，它只会转移

我们可以选择在写入的时候，提前把数据多加工一些，直接加工成前端需要的样子，让前端在查询的时候能不需要等待太久。

就好像小卖铺的老板，总会在学生下课前，提前包装好一盒盒打包好的肠粉，而不是等学生过来点单的时候才开始煮肠粉。

这种写的时候多耗时，让查的时候更轻松的做法，通常被我们叫做写扩散

![img](./10.jpg)

就像这个场景，用户只要总数，如果我提前算好，肯定比用户查询的时候来聚合，速度更快。

面对复杂查询常见的解决方案，就是构建视图表，也是我最近在做的统计sdk要做的事，能够对老表数据构建一张对查询更加友好的统计表，并支持数据初始化，准实时更新，主动刷新。

### 写瓶颈

写瓶颈通常出现在这些场景：

1. 接口限流
2. 秒杀抢购商品
3. 商品列表的接口

针对于这种写的流量非常大，成为了瓶颈，他的解决思路和读不太一样。

常见的解决方案有**异步**，**打散，合并**

#### 异步

对于重要但是不紧急的写入操作，通常可以把它丢到消息队列里面去，晚点再执行。消息队列最大的好处就是削峰填谷。

比如发送短信，比如订单成功后的通知。

但是有两种情况，一种是用户立马就要知道结果，这种紧急操作。还有一种是写入的量一直都这么大，就是没有谷。消息队列永远消费不完。这就只能用到下面两种方案了。

#### 打散

针对写热点，常见的解决方案就是打散。如果热点在单点的数据库，那就做**分库分表**，把流量分散到每个数据库去。

那如果热点在单点呢，就是某个**优惠券的库存**。还能不能做分库分表？依然是打散，不是数据库层面。

可以在把优惠券库存的扣减放到redis，并且设置10个key（redis集群），每个key都存一部分的库存。

![img](./image.png)

如果第一个key没有库存了，就去找第二个key扣，并且扣减的时候随机选取，避免单热点的出现。需要查询总库存的时候，在去聚合10个key的库存。

这种通过打散来应对单热点的做法，也广泛的应用在其他场景。比如`LongAdder`一个**jdk的原子自增类**。内部其实是一个数组的

![img](./1565575-20230217155220890-1495933987.png)

在以往并发情况下进行累加计数时通常使用AtomicLong,因为其底层是基于cas实现的，高并发下对单一变量进行CAS操作，从而保证其原子性。其它的线程都会进行不断自旋，这就产生了问题，随着并发线程数增加，等待线程的自旋时间也会大幅增加，白白造成了CPU资源的浪费；

当线程写base有冲突时，将其写入数组的一个cell中(初始值为0)。将base和所有cell中的值求和就能得到最终LongAdder的值。

#### 合并

借用hystrix的图，合并和打散其实有异曲同工之妙，相当于把流程倒过来了。

![img](./5.png)

合并最大的好处，就是可以减少线程池的开销，减少连接的开销，特别是下游支持合并请求的情况下，合并可以极大的降低请求的写入压力，假设合并10个请求，qps对于下游降了10倍。

使用请求合并技术的开销就是导致**部分请求延迟增加**，因为需要等待一定的时间将多个请求合并起来，但是项目的整体**吞吐量是会上升**的。

比如对于**订单库存的扣减**，每一次请求对应一个sql,`update table set stock=stock-1;`合并后变成请求`update table set stock=stock-10;`对数据库的执行效率是一样的，性能就减了10倍。

对于一个**全局限流中心**，限流的目的，就是怕高qps打爆我们的服务器。结果你做了一个全局的限流中心，所有服务器的请求，都经过限流中心，反而它成为了瓶颈了？？？

一个好的做法，就是请求合并。比如内存合并20次请求，再统一去限流中心上报次数。这种做法看起来不太妥。合并还有另一种运用场景，就是预合并取，比如全局限流中心是令牌桶模式发放的是令牌，每个项目申请的时候，一次性多扣点，比如20个。然后放进内存里，用原子自增类去一个个扣。

这种思路同样运用来**分布式id生成器**里。分布式id，每次生成都要去数据库自增的话，也是个单点的问题。[美团的leaf](https://tech.meituan.com/2019/03/07/open-source-project-leaf.html)

一次从数据库扣减一个号段，然后在内存里一个个去发放，把数据库单点热点，变成了内存的分散发放。

![img](./image1.png)

市面上用到合并请求的思想其实很多，底层到tcp的nagle算法合并请求包；再到kafka的消息发送缓存区；再到[hystrix](https://github.com/Netflix/Hystrix/wiki/How-it-Works#RequestCollapsing)开源的框架，快手开源的[buffer-trigger](https://github.com/PhantomThief/buffer-trigger)框架，市面上的游戏需要即时推送用户的。

### 请求折叠适用场景

请求折叠最大的作用，就是提升吞吐量。提升吞吐量的关键就是打通之前引起阻塞的点，也就是最短的那块木板。并且还需要下游能够支持批量请求。做到N次请求合并成一次，效率能提高N倍。

要求：

1. 下游能够支持批量请求
2. N次请求合并成一次，效率能提高N倍
3. 合并的请求，是明显的热点请求，qps比较高

适用场景：

请求折叠非常适用于对**响应时间要求不高**，希望能够**提升整体吞吐量**的系统，特别适用于在mq的消费场景。

1. 埋点上报系统，单个用户触发的多个埋点，在客户端合并后，以固定频率发送给服务端上报。
2. 优惠券的批量发放任务，或者热门商品的秒杀库存。
3. push的全量推送（如果运营商支持批量推送，可以极大提升吞吐量）

我们公司的消息推送系统，要统计每个推送的到达量和点击量，每次大促一推送出去，就会有非常多的点击上报，对数据库压力非常大，我们做了一个请求合并后，压力瞬间下降，效果很好。

![img](./image-1703648880078.png)

```bash
@HystrixCollapser(batchMethod = "updateAdminPushBatch", scope = Scope.GLOBAL
        , collapserProperties = {@HystrixProperty(name = "timerDelayInMilliseconds", value = "200")
        , @HystrixProperty(name = "maxRequestsInBatch", value = "100")})
    public Future<Boolean> increase(ReportData data) {//单个请求
        log.debug("push report increase data:{}", data);
        return null;
    }
    @HystrixCommand(commandProperties = {
        @HystrixProperty(name = "execution.isolation.thread.timeoutInMilliseconds", value = "5000")
    })
    public List<Boolean> increaseBatch(List<ReportData> dataList) {//合并请求
        //按照推送模板纬度分组
        Map<String, List<ReportData>> mapGroup = dataList.stream().collect(Collectors.groupingBy(ReportData::getPushId));
        mapGroup.forEach((key, value) -> {
            int clickCount = 0;//点击量
            int arrivedCount = 0;//到达量
            String pushId = key;
            for (ReportData data : value) {
                final String eventType = data.getEventType();
                if (Objects.equals(eventType, CLICK_COUNT_KEY)) {
                    clickCount++;
                } else if (Objects.equals(eventType, ARRIVED_COUNT_KEY)) {
                    arrivedCount++;
                }
            }
            //对数据库做上报增加
            dao.increase(pushId,clickCount,arrivedCount);
        });
        return dataList.stream().map(m -> true).collect(Collectors.toList());
    }

    @Data
    @AllArgsConstructor
    public static class ReportData {
        private Long uid;
        private String deviceId;
        private String pushId;
        private String eventType;
    }
```

### 自己开发一个请求折叠工具

如果我们需要有一个合并请求的框架，它需要满足哪些点呢？

![img](./请求合并初稿 .jpg)

1. 能够对**请求分组**，比如游戏推送用户，对单个用户的消息推送才合并。
2. 要有多个**队列**临时缓存这些待合并的请求，这些队列要支持**并发**
3. 灵活配置，比如根据**逗留时间**和**容量阈值**，触发合并请求
4. 可能要有**线程池**，来进行合并请求的执行
5. 能够返回单次**请求结果**

## buffertrigger实现

https://blog.csdn.net/m0_73311735/article/details/127668514

1.实现的功能，背压模式，背压简单来说就是处理能力不够的时候，让上游感知到能力，用拒绝策略来实现上游投递的阻塞

2.有类似rdb的快照模式，靠定时任务时间，定时任务时间为两次快照时间的最小间隔

3.simplebuffertrigger，提供了扩展点，不够用了，就继承buffertrigger实现其他框架。

4.用builder构造实例的好处体现

### 缺点：

1.拿不到返回的结果

2.单线程

3.没有分组，不适合分组批量聚合的场景

## kafka实现

首先kafka有个RecordAccumulator，来实现消息的批量发送。

```bash
<dependency>
    <groupId>org.springframework.kafka</groupId>
    <artifactId>spring-kafka</artifactId>
    <version>2.5.10.RELEASE</version>
</dependency>
```

![img](./09100800_63e455802ea8f26395.png)

通过

batch.size=16kb

linger.ms=10

就可以配置根据数量或者大小的批量发送

### 实现

![img](./image-1703648880079.png)

1.添加消息的地方

org.apache.kafka.clients.producer.internals.RecordAccumulator#append

![img](./image-1703648880078.png)

如果满了只是修改了batch的标志，并没有主动发送。

并且返回的result内含future对象，是等发送成功或者失败后，设置回future的值

![img](./2.png)

成功后设置的位置

org.apache.kafka.clients.producer.internals.ProducerBatch#done

2.发送消息的地方

org.apache.kafka.clients.producer.internals.Sender#runOnce

![img](3.png)

sender对象创建的时候，就会运行一个线程，循环调用runOnce。从accumulator里面取等待超时的批次，或者已经满了的批次。

org.apache.kafka.clients.producer.internals.RecordAccumulator#ready

![img](./image-1703648880079.png)

一次扫描，只取一个队列里面的头批次，这是为什么？kafka也给出了原因。

![img](./4.png)

有时候很多topic和很大分区，这样做减少扫描花费的时间，来保证单分区的发送量大，不要影响到其他的topic分区。

这里同时计算下一次扫描的最短时间，来触发下一次扫描。等待的时间，去用来调用selector，等待netty事件了。

### 优点：

1.支持分组通过map的方式存放

2.支持返回结果，返回future，成功后设置进future。

### 缺点：

1.满了后并不会直接发送，还是等下一次扫描

思考：可能是kafka对于消息的处理速度，要求没有那么严格。它更在乎整体的吞吐量。

2.没有为分组消息分配线程池

因为kafka都是单线程进行消息的发送，没有这方面的需求。

## hystrix实现

https://github.com/Netflix/Hystrix/wiki/How-it-Works#RequestCollapsing

```bash
 <dependency>
    <groupId>com.netflix.hystrix</groupId>
    <artifactId>hystrix-core</artifactId>
    <version>1.5.12</version>
</dependency>
```

![img](./5.png)

### 实现细节

后端的合并器，是RequestCollapser

具体的分批是RequestBatch

![img](./image-1703648880080.png)

提交请求的方法是com.netflix.hystrix.collapser.RequestCollapser#submitRequest

首次初始化会添加一个定时任务，也就是根据指定时间来触发批量执行的任务。

![img](./image-1703648880080.png)

接入的用户需要写子类，实现这些方法

![img](./image-1703648880081.png)

里面包括触发的请求，和结果的转换。

优点

1.没有额外的线程池开销，总数量的触发全靠当前线程

2.也完美的达到了定时任务指定时间的触发请求

缺点

1.没有分组功能

## collapse-executor

自闭少年（阿里基础架构组（负责dubbo等框架开发的大佬），了解各大框架，性能优化）的项目https://github.com/icodening/collapse-executor

![img](./6.png)

项目比较大，自行领悟，**适合想进阶的人**。

用到了无线程线程池的概念ThreadlessExecutor

用到了各种合并策略，比如通过线程让步，来进行合并SuspendableCollector



## 抹茶项目的请求折叠器

抹茶之后也会封装一套请求折叠器，教大家从0到1实现一个符合项目需求的折叠器，再加上对这些开源折叠器的分期，形成一套组合拳，又是一个大亮点，大家可以期待下。

## 总结

通过对高并发的读写瓶颈的介绍，相信大家都能带到平常见到的大厂设计方案，或者框架的实现方案中，找到对应的例子。没找到也没关系，上面介绍了很多框架，去了解后就是你的了。

以后面对场景设计题，大家要基本有个思路，怎么去解决问题。当然场景题是广泛的考核，除了性能优化，还会靠你分布式下的事务，一致性，可靠性等问题，需要综合的去准备。

另外就是对请求折叠这个专题，进行深入的展开。你会调研到市面上大部分的折叠器的实现，他们的优缺点。一直引出到你为此做了个更全面，更符合业务的折叠器（超级亮点）。

当面试官问你，你回答：

这些年工作中和项目中，自己形成了一套对问题解决的方案的思考思维模式，牵扯出**读写热点的解决方案**，再引入你项目中很多场景能用到**请求折叠**，比如抹茶的消息推送。你为了做这个，去**调研了哪些框架**，面试官会问，为什么重复造轮子？？？因为**每个框架都有它的不足**，这是你**阅读源码发现**的。你做了一个更符合业务场景的**框架sdk**，甚至还**封装了starter**，进行了**压测效果对比**。并且这个框架用到了**JUC**的各种工具交互，说明你对它的理解之深，再牵扯出juc的八股文。

吊打中厂大多数面试官

但是，成功的秘诀是勤奋，这些都需要自己去亲身经历一遍，思考一遍。**比如阿斌就花了一整天，看完了这几个框架源码**。需要的前置知识，都需要自己去深入学习。比如juc可以参考《阿斌java之路》，架构设计参考《架构之路》。

后续抹茶实现了这个工具类，会有更详细的文章。



## 参考文献:

少年阿斌:https://www.yuque.com/snab/architecture/mqtmi1d5i07tm0w1?singleDoc# 